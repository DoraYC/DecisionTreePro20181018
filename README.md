
# 决策树
## 决策树模型

* 定义

分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性，叶结点表示一个类。

* 决策树与if-then 规则

可以将决策树看成一个if-then规则的集合。决策树的路径或者其对应的if-then规则集合具有一个重要的性质：**互斥并且完备** 。这就是说，每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。

* 决策树与条件概率分布

决策树还表示给定特征条件下类的**条件概率分布**。这一条件概率分布定义在特征空间的一个划分（partition）上。将特征空间划分为互不相交的单元（cell）或区域（region），并在每个单元定义一个类的概率分布就构成了一个条件概率分布。

* 决策树学习

决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树可能有多个，也可能一个也没有。我们需要的是一个与训练数据集矛盾较小的决策树，同时具有很好的泛化能力。从另一个角度看，决策树学习是由**训练数据集估计条件概率模型**。基于特征空间划分的类的条件概率模型有无穷多个。我们选择的条件概率模型应该不仅对训练数据有很好的**拟合**，而且对未知数据有很好的**预测**。

决策树学习的损失函数通常是**正则化的极大似然函数**。决策树学习的策略是以损失函数为目标函数的最小化。

决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据集进行分割，使得对各个子数据集有一个最好的分类的过程。

决策树学习算法包含特征选择、决策树的生成与决策树的剪枝过程。


## 特征选择

* 特征选择问题

特征选择是决定用哪个特征来划分特征空间。
通常特征选择的准则是信息增益或信息增益比。

* 信息增益（information gain）

熵（entropy）是表示随机变量不确定性的度量。（信息的期望值）

条件熵*H(Y|X)* 表示在已知随机变量*X* 的条件下随机变量*Y* 的不确定性。

信息增益表示得知特征*X* 的信息而使得类*Y* 的信息的不确定性减少的程度。（在划分数据集之前之后信息发生的变化）

一般地，熵*H(Y)* 与条件熵*H(Y|X)* 之差称为互信息（mutual information）。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。

* 信息增益比

以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比（information gain ratio）可以对这一问题进行校正。这是特征选择的另一准则。


## 决策树的生成

* ID3算法

ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。
ID3相当于用极大似然法进行概率模型的选择。
ID3算法只有树的生成，所以该算法生成的树容易产生过拟合。

* C4.5的生成算法

C4.5算法与ID3算法相似，C4.5算法对ID3算法进行了改进。C4.5在生成的过程中，用信息增益比来选择特征。



## 决策树优缺点

* 优点

计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。

* 缺点

可能会产生过度匹配问题。

* 适用数据类型

数值型和标称型
